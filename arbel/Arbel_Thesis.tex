\documentclass[12pt]{article}

\relpenalty=9999
\binoppenalty=9999

% Information

\title{\textbf{Generically Increasing Bounded Independence}}
\author{Arbel Deutsch Peled}


%Basic document settings

\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\usepackage{amsfonts,amsthm,amsmath,amssymb}
\usepackage{bm}
\usepackage{mathtools}
\usepackage{array}
\usepackage{epsfig}
\usepackage{fullpage}
\usepackage{amssymb}
\usepackage{soul,color}
\usepackage{float}
\usepackage[boxed, linesnumbered,]{algorithm2e}
\usepackage[toc,page]{appendix}
\usepackage{smartdiagram}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{polyglossia}
\usepackage{csquotes}

\usepackage{fontspec}
\setmainfont{Times New Roman}

\usepackage{tikz}
\usetikzlibrary{arrows, decorations.markings, positioning, calc}
\tikzset{
	treenode/.style = {shape=rectangle, rounded corners,
		draw, align=center,
		top color=white, bottom color=blue!20},
	root/.style     = {treenode, font=\Large, bottom color=red!30},
	env/.style      = {treenode, font=\ttfamily\normalsize},
	dummy/.style    = {circle,draw}
}

\setdefaultlanguage{english}
\setotherlanguage{hebrew}

\renewcommand{\contentsname}{Table of Contents}

% Bibliography
\usepackage[backend=biber,style=alphabetic,sorting=ynt]{biblatex}
\DefineBibliographyStrings{english}{%
	references = {Bibliography}
}
\addbibresource{bibliography.bib}

\setlength{\parskip}{0.5em}


% Global definitions

\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{
	\newenvironment{rep#1}[1]{
		\def\rep@title{#2 \ref{##1}}
		\begin{rep@theorem}}
		{\end{rep@theorem}
	}
}



\newtheorem{definition}{Definition}[section]
\newtheorem{assumption}{Assumption}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{note}[theorem]{Note}
\newreptheorem{lemma}{Lemma}
\newreptheorem{theorem}{Theorem}


\newcommand{\removelatexerror}{\let\@latex@error\@gobble}
\def\tagform@#1{\maketag@@@{(\ignorespaces#1\unskip\@@italiccorr)}}

\makeatother

\newcommand{\IGNORE}{\textbf{\hl{THIS SECTION IS A DRAFT}}\par}

% Useful definitions

\newcommand{\Span}[1]{\mathrm{Span}\left(#1\right)}
\newcommand{\Supp}[1]{\mathrm{Supp}\left(#1\right)}
\newcommand{\Rank}[1]{\mathrm{Rank}\left(#1\right)}
\newcommand{\Poly}[1]{\mathrm{poly}\left(#1\right)}
\newcommand{\Polylog}[1]{\mathrm{polylog}\left(#1\right)}
\newcommand{\bias}[2]{\mathrm{bias}_{#1}\left(#2\right)}
\newcommand{\zo}[1]{\{0,1\}^{#1}}
\newcommand{\oo}[1]{\{-1,1\}^{#1}}
\newcommand{\dist}{\mathcal{D}}
\newcommand{\ADD}{\mathrm{ADD} }
\newcommand{\AND}{\mathrm{AND} }
\newcommand{\BOOST}{\mathrm{BOOST} }
\newcommand{\enc}{\mathrm{Enc} }
\newcommand{\FILT}{\mathrm{FILT} }
\newcommand{\OR}{\mathrm{OR} }
\newcommand{\XOR}{\mathrm{XOR} }
\newcommand{\XIST}{\mathrm{XIST} }
\newcommand{\PROJ}{\textbf{PROJ} }
\newcommand{\PROJO}{$\textbf{PROJ}_1$ }
\newcommand{\PROJT}{$\textbf{PROJ}_\tau$ }
\newcommand{\proj}[3]{\mathbf{PROJ}_{#1, #2} \left( #3 \right) }
\newcommand{\ANDXOR}{\textbf{AND-XOR} }
\newcommand{\NOT}{\textbf{NOT} }
\newcommand{\GFT}{\mathrm{GF}\left(2\right)}
\newcommand{\sut}{\, \\dist_i \,}
\newcommand{\ih}{: \:}
\renewcommand{\B}{\mathcal{B}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\F}{\mathcal{F}}
\renewcommand{\P}{\mathcal{P}}
\renewcommand{\T}{\mathbb{T}}
\newcommand{\calU}{\mathcal{U}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\rand}{\overset{R}{\leftarrow}}
\newcommand{\defi}{\overset{def}{=}}
\newcommand{\abs}[1]{\left| #1 \right|}
\newcommand{\mat}[1]{\mathbf{#1}}
\renewcommand{\vec}[1]{\bm{#1}}
\newcommand{\rows}[1]{\mathrm{Supp}\left({#1}\right)}
\newcommand{\wt}[1]{\mathrm{wt}\left(#1\right)}
\newcommand{\var}{\mathrm{Var}}
\newcommand{\cov}{\mathrm{Cov}}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}

\smartdiagramset{
	uniform color list=gray!60!black for 4 items,
	back arrow disabled=true
}

% Document begins

\begin{document}

\begin{titlepage}
	\centering
	\includegraphics[width=0.4\textwidth]{IDClogo.png}\par\vspace{2cm}
	{\huge The Interdisciplinary Center, Herzliya \par}
	{\Large Efi Arazi School of Computer Science \par}
	{\Large M.Sc. Program - Research Track \par}
	
	\vspace{1cm}
	
	\vspace{1.5cm}
	{\Huge Generically Increasing Bounded Independence\par}
	\vspace{3cm}
	{\large by\par}
	{\large\bfseries Arbel Deutsch Peled\par}
	
	\vspace{2cm}
	
	{M.Sc. dissertation, submitted in partial fulfillment of the requirements\par}
	{for the M.Sc. degree, research track, School of Computer Science\par}
	{The Interdisciplinary Center, Herzliya}
	
	\vfill
	
	% Bottom of the page
	{\large September 2016\par}
\end{titlepage}

\newpage

This work was carried out under the supervision of Alon Rosen.
It is based on joint work with Andrej Bogdanov and Alon Rosen.

\newpage

\section*{Abstract}
	
	Almost $k$-wise independent hash functions are function families whose outputs on any $k$ distinct queries are close to uniform in $L_1$ distance; this is also called \emph{bounded independence}.
	A close relative, called almost $k$-wise unbiased functions, is a family of functions whose outputs on any $k$ distinct queries are close to uniform in $L_\infty$ distance.
	
	In this work we explore methods for increasing the bounded independence of a family of hash functions.
	Namely, given an almost $k$-wise independent (unbiased) function family, we aim to produce an almost $k'$-wise independent (unbiased) one for $k' > k$.
	Our transformations are generic in the sense that they only require black-box access to the underlying hash function families, and in most cases only require these to be almost $k$-wise independent without any further restrictions.
	To the best of our knowledge, no such method was known before.
	
	In order to achieve our goals we employ the following method: repeatedly sample from the original function family and define a new function that is some combination of the samples.
	We identify two types of predicates with which to combine the sampled functions: one type will allow us to decrease the bias of the output; the second type will allow us to increase the bounded independence parameter $k$.
	We finally combine the two types of predicates in an iterative construction which has the required properties.

\newpage

\tableofcontents

\newpage

\section{Introduction}
	
	Hash functions are a fundamental element of modern computer science.
	They have been the subject of extensive research dating back to the 1950s and have seen practical use in diverse settings (see section \ref{section: related work}).
	They come in many flavors, which can be roughly divided into two types: information-theoretic functions and cryptographic ones.
	
	In 1979, Carter and Wegman \cite{Original} defined the concept of \emph{Universal-Hashing}, which started a vast array of publications on this topic.
	Roughly speaking, Universal Hash Families guarantee that the function outputs are pairwise-independent.
	This notion was later extended by the same authors to $k$-wise independence.
	Families of functions whose outputs are $k$-wise independent are called $k$-Universal, or simply $k$-wise independent.
	
	In simple words, this definition means that the distribution induced by the hash function family over the outputs of up to any $k$ distinct inputs is completely uniform.
	This notion can be relaxed a bit without losing too much of its power if we only require that the distribution over output tuples, is statistically close to uniform in $L_1$ norm.
	That is to say, if we consider distributions over $\zo{k}$ as vectors in $\mathbb{R}^{2^k}$ then the $L_1$-norm of the difference between the output distribution and the uniform one is small.
	Families of hash functions that satisfy this constraint are said to be almost $k$-wise independent.
	
	This notion provides a very strong guarantee on the $k$-tuple of outputs.
	It basically says that the distinguishing advantage of any (even unbounded) adversary between the $k$ outputs of such a hash family and $k$ completely random elements from the function family's range is at most $\epsilon/2$.
	
	A less stringent requirement is that the function family have small bias.
	We say that a function has small bias if any linear test over the components of the output distribution is close to uniform in expectation.
	Families of functions (or distributions) which exhibit this property are said to be almost-unbiased.
	
	It turns out that these two notions are actually related to one another.
	In \cite{VaziraniBias}, it was shown that an exponentially small bias can be translated into a bound on $L_1$ distance.
	This was later improved upon in \cite{DiaconisBias}. Specifically:
	\begin{lemma}[Diaconis-Shahshahani lemma restated] \label{lemma: diaconis bias}
		If $\dist$ is a distribution over $\{0,1\}^k$, whose bias with respect to any linear test is at most $\epsilon$: then this $\dist$ is also at most $2^{k/2} \epsilon$-far from the uniform distribution in $L_1$ norm.
	\end{lemma}
	
	Throughout this text, we will be using the notion of bias, rather than that of independence, which will turn out to be more natural to our analysis.
	These results can then be transformed via lemma \ref{lemma: diaconis bias} into the language of almost $k$-wise independence.
	In some cases we will specify the implications regarding almost $k$-wise independence explicitly, but in others we will let these results remain implicit. 
	
	\subsection{Our Results}
	
	We consider function families which have small $\epsilon$ bias w.r.t. all linear tests of size at most $t$, where by size we mean the number of components in the output distribution that participate in the test.
	Such families are called $(t, \epsilon)$-biased.
	In this thesis we present a generic method for transforming a $(t, \epsilon)$-bias function family into a $(t', \epsilon')$-biased one while only using black-box access to the original family.
	Our constructions do not change the input or output length of the original family.
	
	We have several motivations for doing so.
	The first is that, while explicit constructions for $k$-wise independent hash functions exist, it is plausible that in some applications a $k$-wise hash function will be inherent to the problem, but this class of functions will not admit a natural method for increasing its independence parameter.
	In this case using a generic transformation may be useful.
	
	A second, and possibly more important, motivation is in creating almost $k$-wise unbiased functions which can be computed by small formulae.
	It has been shown in \cite{NaturalProofs} that pseudo-random functions cannot be instantiated in complexity classes that admit Natural proofs.
	Our smallest constructions lie very close in size to the current known bound for pseudo-random function (PRF) formulae.
	One may conjecture that almost $k$-wise independent hash functions are information-theoretic relatives of the computational PRFs.
	Therefore we hope to construct functions which may not only be almost $k$-wise unbiased but also valid PRF candidates.
	
	In section \ref{section: generic constructions} we prove the following theorem:
	\begin{reptheorem}{theorem: simplest hash enhancement}[loosely stated] \label{theorem: basic theorem}
		Let $\F_0 \subseteq D \rightarrow \{0,1\}$ be a $(k_0, \epsilon_0)$-biased function family for some constants $k_0 \geq 2$ and $\epsilon_0 < 1$.
		Then it is possible to efficiently construct a family $\F \subseteq D \rightarrow \{0,1\}$ that is $(k, \epsilon)$-biased by deterministically combining at most a polynomial (in $k$ and in $\log(\frac{1}{\epsilon})$) number of independent samples from $\F$.
	\end{reptheorem}
	We first state and prove this theorem for hash functions which hash bit strings into single bits.
	In section \ref{section: reducing randomness} we generalize this to arbitrary input and output sizes.
	
	This theorem follows from two combination lemmas of complementing nature.
	We show that XOR-ing two samples from a hash function family reduces bias, while AND-ing two samples effectively doubles the independence parameter of the function family while, unfortunately, increasing the bias.
	Alternating between these two combination methods will allow us to increase the independence parameter while controlling the bias.
	This process is roughly sketched in figure \ref{figure: single independence step}.
	In section \ref{section: composition lemmas} we state and prove these two lemmas.
	
	\begin{figure}[H]
		\centering
		\tikzstyle{level 1}=[level distance=3.0cm, sibling distance=3.5cm]
		\tikzstyle{level 2}=[level distance=2.5cm, sibling distance=2cm]
		\tikzstyle{level 3}=[level distance=2.5cm, sibling distance=1cm]
		\begin{tikzpicture}
		[
		grow                    = left,
		sibling distance        = 6em,
		level distance          = 6em,
		edge from parent/.style = {draw, >=latex},
		every node/.style       = {font=\footnotesize},
		sloped
		]
		\node [root] {$(2k,\frac{1}{8})$-biased}
		child { node [dummy] {XOR}
			child { node [env] {$(2k, \frac{7}{8})$-biased}
				child { node [dummy] {AND}
					child { node [env] {$(k, \frac{1}{8})$-biased}
						edge from parent node [above, align=center]{} }
					child { node [env] {$(k, \frac{1}{8})$-biased}
						edge from parent node [above, align=center]{} }
					edge from parent node [above] {}}
				edge from parent node [above, align=center]{} }
			child { node [env] {$(2k, \frac{7}{8})$-biased}
				child { node [dummy] {AND}
					child { node [env] {$(k, \frac{1}{8})$-biased}
						edge from parent node [above, align=center]{} }
					child { node [env] {$(k, \frac{1}{8})$-biased}
						edge from parent node [above, align=center]{} }
					edge from parent node [above] {}}
				edge from parent node [above, align=center]{} }
			edge from parent node [above] {} };
		\end{tikzpicture}
		\caption{Sketch of a single independence increasing step} \label{figure: single independence step}
	\end{figure}
	
	This latter process is the heart of our constructions.
	It is the means by which we increase the titular bounded independence.
	However, it cannot work on its own; rather, it is the second stage of three.
	
	In the first stage the bias is reduced down to a "small enough" constant using repeated XORs.
	This is necessary in order to be able to bound the bias in an AND step.
	The third and final stage is also a bias-reduction one, which is needed since the second stage only provides a constant bias.
	In most cases we will require sub-constant, and even exponentially small, final bias.
	
	Applying theorem \ref{theorem: basic theorem}, we define a new type of hash-function family which is most easily described as a formula, i.e. a circuit in which every gate has fan-out 1.
	Our construction follows in the footsteps of \cite{Valiant}.
	In this paper the majority function of $n$ bits is shown to be calculable by a monotone formula which uses just one type of gate.
	The original construction works by sampling with replacement $m>n$ bits from the input and then calculating a formula built of a balanced tree containing only majority gates.
	
	Our construction will use the same sampling technique, but employ two types of gates: XOR and AND, as in our theorem.
	By proving that the sampling procedure is effectively an almost pairwise-independent hash function we will be able to directly apply our theorem and show that a suitable (explicit) formula over a suitably long series of independent samples from the input is almost $k$-wise unbiased for desired value of $k$ and distance from uniformity.
	This is detailed in section \ref{section: Simple circuit construction}
	
	We then turn our attention to optimizing the performance of the construction.
	In this endeavour we have two conflicting goals:
	\begin{enumerate}
		\item Reduce the randomness used in the construction
		\item Reduce the construction's formula size
	\end{enumerate}
	In addition, we may hope to support larger outputs and more general inputs (i.e. not just bit strings).
	
	The first goal is motivated primarily by practical reasons: in many applications it is important to save on randomness.
	We explore this goal in section \ref{section: reducing randomness}.
	First, we improve upon the efficiency of the AND gate.
	In order to do this we first need to generalize our construction to handle longer outputs.
	We treat our hash functions as families $\F : D \rightarrow \Z_q$.
	At this point we replace the AND gate with a different one which enables us to square the independence parameter $k$ instead of just doubling it.
	This drastically reduces the required randomness.
	We also show that addition modulo $q$ generalizes the XOR bias-reduction operation.
	
	Second, we optimize the final bias-reduction stage.
	We employ the well-known technique of using random walks on expanders instead of independently sampling, first used in \cite{RandomWalk}.
	In lemma \ref{lemma: random walk} we show that XOR-ing together samples taken from a random walk on an expander graph is almost as good at reducing bias as XOR-ing completely independent samples from the same graph.
	Using this idea, we will be able to reduce the cost of a single XOR gate, which na\"{i}vely would require doubling the randomness, to an additive constant number of bits.
	These two optimizations are generic, and can be applied to almost any hash function family.
		
	In section \ref{section: general construction} we first derive a general transformation from $(k_0, \epsilon_0)$-bias to $(k,\epsilon)$-bias using the improvements of the previous section.
	This implies a similar transformation for $k$-wise independence.
	The parameters of our most randomness-efficient constructions are then shown to be only linearly dependent on the final required value of $k$, and logarithmically dependent on the size of the functions' range.
	
	We then generalize our simple circuit construction from section \ref{section: generic constructions} to allow for multiple output bits, and show how the new theorems relate to this construction.
	This is done by sampling multiple bits from the input at random to create a single bit-string output.
	We then show a further simple optimization of this construction using error-correcting codes.
	Specifically, we show that by applying a suitable code on the input prior to sampling from it, we can reduce the bias of the bit-string output to a constant.
	This helps to reduce the amount of randomness required by the initial bias-reduction stage.
	
	In section \ref{section: reducing formula size} we pursue a tangent direction: improving the formula size of our original $\zo{n} \rightarrow \{0,1\}$ hash function family from section \ref{section: Simple circuit construction}.
	It has been shown in \cite{NaturalProofs} that PRFs cannot exist in complexity classes that have a Natural Property.
	In \cite{GeneralFormulaLowerBound} a Natural proof was shown giving a lower bound of $O(\frac{n^2}{\log n})$ for the formula size of any function computing the so-called \emph{selection} function.
	Furthermore, in the case of De-Morgan formulae, in a series of papers (\cite{Andreev87}, \cite{Impagliazzo93} and \cite{Hastad98}) a Natural proof was shown giving a lower bound of $O(n^{3 - o(1)})$ for the size of such a formula computing some specific function.
	In this discussion we refer to \emph{general formulae} as ones comprised of AND, OR, XOR and NOT gates.
	De-Morgan formulae are restricted to AND, OR and NOT gates.
	
	We show constructions of almost $n$-unbiased hash functions which can be implemented by general formulae of size $\tilde{O}(n^2)$ and by De-Morgan formulae of size $\tilde{O}(n^4)$.
	By doing so we hope to incentivize further exploration of this kind of formulae, which may result in either a stronger support for the possibility of constructing such PRFs or, alternatively, in finding Natural proofs of this size.
	Our results in this area are summarized in theorem \ref{theorem: Small-Sized Formulae}.
	
	\begin{theorem} \label{theorem: Small-Sized Formulae}
		There exists a family of $(n,2^{-n})$-biased hash functions $\mathcal{F}: \zo{n} \rightarrow \{0,1\}$ whose formula size is:
		\begin{enumerate}
			\item $O(n^2 \log^2 n)$ for general formulae, and
			\item $O(n^4 \log^2 n)$ for De-Morgan formulae
		\end{enumerate}
	\end{theorem}
	
	In order to improve upon the na\"{i}ve construction's formula size we employ two additional techniques.
	Reducing the starting bias is done using a well-known technique for generating pairwise independent distributions from slightly unbiased ones.
	Namely, we compute the inner product between the input and a uniformly random vector over the same space.
	Then we replace our generic, iterative independence-amplification technique with a more standard, but less generic, lemma from \cite{ValVaz}.
	The final stage of the construction, the bias-reduction stage, remains untouched.
	We comment that this seems to be the main bottleneck in any attempt to increase bounded independence.
	
	\subsection{Related Work} \label{section: related work}

	$k$-wise independent hashing is very useful in a wide array of fields. To cite a few:
	It has been shown by \cite{5LinearProbing} that 5-wise independent hash families provide optimal expected time-complexity for adding key-value pairs to hash tables using linear probing.
	In Cuckoo Hashing introduced by \cite{CuckooHashing}, when storing $n$ key-value pairs, it is required to have 2 independently chosen $\log n$-wise independent hash functions in order to provide good analytical guarantees for the expected performance of the system.
	In \cite{4UniversalSecondMoment} an online algorithm is shown that estimates the second moments of a stream of data, using $4$-wise independent hash functions.
	Another theorem proved in \cite{StorageEnforcing} states that (almost) $2$-wise independent hash families are Storage Enforcing, meaning that they allow a verifier to ascertain that their data is indeed stored on a server by just saving some small hash of the data.
	
	The standard technique for generating $k$-wise independent hash functions is the original one introduced in \cite{K-Indenepdence}.
	In that paper, in was shown that the family of hash functions defined by all polynomials of degree at most $k$ over some finite field $F_p$ with $p > k$ (sampled uniformly at random) yields a $k$-wise independent hash function family.
	This construction is very efficient in terms of key size ($k \log p$).
	Our most efficient constructions have a key size larger by a constant factor from this result.
	Furthermore, in the case when we are only interested in small bias (as opposed to independence): the key size required by the polynomial-based construction is much larger than the one used by the constructions presented here.
	
	Another good property of the standard construction is that it is also exactly (as opposed to almost) $k$-wise independent.
	However, it does not offer any insight into how to combine arbitrary hash function families in order to improve their parameters, which is the main concern of this thesis.
	
	Another common method, which has seen wide application in practice, for generating $k$-wise independent hash families is 
	Tabulation Hashing.
	This approach, which has many derivatives (e.g. \cite{4UniversalSecondMoment}, \cite{Twisted}), stores several "small" copies of truly random hash functions over a smaller domain.
	These functions are stored as input-output tables, which give this method its name.
	When queried with some input: the function splits it into several smaller parts, queries each table using its respective part of the input and then combines them in a deterministic way.
	This sort of hash function is only useful for small $k$ and constant input length, as it needs to randomly draw and then store tables of exponential size.
	
	The usefulness of these functions comes from their time-efficiency: each one only requires a constant time to evaluate, and this constant is usually very small.
	However, from an asymptotic point of view: they are prohibitive.
	Furthermore, it was shown in \cite{Concentrators} that constant evaluation time can only be achieved using exponentially-large storage.
	Our constructions have logarithmic evaluation time, and only use memory of size polynomial in the output length and independence parameter $k$.
	
	In our work we use the notion of bias both for its own sake and in order to imply almost-independence.
	The importance of small-bias distributions was first shown in \cite{SmallBias}, in which probability spaces with this property are constructed using a low amount of entropy.
	In the following text we make use of the same definitions for bias but view them from a different perspective.
	When we consider $k$ outputs of a hash function over $k$ distinct inputs, we must assume that the hash function is defined in the same way for each of these input/output pairs.
	Moreover, the $i$-th output cannot depend on the $j$-th input, for any $i \neq j$, since this would imply the construction is not a function.
	This restriction, which is not present in the setting of the original paper, means that our constructions would be hard-pressed to compete with the original ones in terms of key-size.
	In fact, they are a different kind of object altogether.
	
	The main topic of this thesis is the introduction of methods that can be used to increase bounded independence.
	Recently and independently, a similar result was shown in \cite{Gowers}.
	In that paper, distributions over $SL(2,q)^m$ are considered.
	It was shown that a component-wise product of $2^{\Omega(m)}$ pairwise independent distributions over that domain forms a distribution that is $\frac{1}{\abs{SL(2,q)}}$-close to uniform on $SL(2,q)^m$.
	The authors of that paper achieve this result via an iterative process which takes some constant number of almost $t$-independent distributions and outputs an almost $(t+1)$-independent one.
	The motivation in that case was completely different, and the increase in independence was the means to a very specific end, which we refrain from mentioning for brevity.
	
	In contrast, our methods allow for a doubly-exponential increase in the parameter $t$ using a small constant number of  samples from a $(t, \epsilon)$-biased distribution, for a suitably small $\epsilon$.
	This means that the same kind of results can be achieved using a much lower number of samples from the original distribution. Our results are also more generic since they work over any group $\Z_q$.
	They are not, however, directly applicable to the objects considered in \cite{Gowers}, since in that case there is no control over which function to use when combining the different samples.

\section{Preliminaries}
	
	\subsection{Notation}
	
	\subsubsection{General}
	
	We will denote scalars in lower-case (e.g $x$) and vectors in \textbf{bold} (e.g. $\vec{x}$ or $\vec{X}$).
	Distributions will be designated by a calligraphic font (e.g. $\dist$).
	Random variables will be denoted by upper-case letters (e.g. $X$ or $\vec{X}$).
	
	The uniform distribution over $n$ bits will be denoted by $\mathcal{U}_n$.
	The uniform distribution over a set $S$ will be denoted by $\mathcal{U}_S$.
	When the domain is obvious, we will sometimes simply write $\mathcal{U}$.
	
	For any natural number $n \in \mathbb{N}$ we denote by $\left[n\right]$ the set $\{1,2,3,\dots,n\}$.
	
	If $\vec{x}$ is a vector, $i$ is an integer and $S$ is a set of integers then we'll denote by $v_i$ the value of $\vec{x}$ in the $i$-th coordinate and by $\vec{x_S}$ the restriction of $\vec{x}$ to the coordinates in $S$. 
	
	\subsubsection{Distributions and Hash Functions}
	
	For a distribution $\dist$ over domain $D$ we denote for any element $x \in D$ its probability of being drawn from $\dist$ by: $\dist(x) = \Pr_{X \sim \dist} \left[ X = x \right]$.
	
	\begin{definition}
		The \emph{support} of $\dist$: $\Supp{\dist} = \{x \in D \, \vert \, \dist(x) > 0\}$ is the set of elements in the domain which have non-zero probability of being drawn from $\dist$.
	\end{definition}
	
	The following form the basic definitions of the properties we expect our hash families to have.
	
	\begin{definition}[Distribution induced by a Function Family]
		Let $\mathcal{F} \in D \times \zo{r} \rightarrow R$ be some family of functions.
		We define the \emph{distribution induced by $\mathcal{F}$ and inputs
			$\vec{x} = \{x_1,x_2,\dots,x_k\} \in D^k$},
		$\dist_{\mathcal{F},\vec{x}} : \: R^k \rightarrow \left[0,1\right]$,
		as a probability distribution over the outputs of the hash function:
		\begin{equation*}
			\dist_{\F,\vec{x}} (\vec{y}) = \Pr_{\rho \overset{R}{\leftarrow} \zo{r}} \left[\forall i : \: \F(x_i, \vec{\rho}) = y_i \right]
		\end{equation*}
	\end{definition}
	
	\begin{definition}[$L_1$ distance for distributions]
		Let $\dist_1$ and $\dist_2$ be two probability distributions over the same domain $D$.
		Then the $L_1$ distance between the distributions is defined as:
		\begin{equation*}
		\abs{\dist_1 - \dist_2}
		= \sum_{x \in D} \abs{\dist_1(x) - \dist_2(x)}
		= \sum_{x \in D} \abs{ \Pr_{X \overset{R}{\sim} \dist_1} \left[ X=x \right] - \Pr_{X \overset{R}{\sim} \dist_2} \left[ X=x \right]}
		\end{equation*}
	\end{definition}
	
	\begin{definition}[Almost $k$-Wise-Independent Hash Function Family]
		A family of functions \break $\F : D \times \zo{r} \rightarrow R$ is said to be $(k,\epsilon)$\emph{-Wise-Independent} if
		for any set of $k$ \emph{distinct} inputs $\vec{x} = \{x_1,x_2,\dots,x_k\} \in D^k$, it holds that:
		\[
			\abs{ \dist_{\F,\vec{x}} - \mathcal{U}_{R^k} } \leq \epsilon
		\]
		
	\end{definition}
	
	In order to analyze the behaviour of a family of hash-functions on several inputs we extend the definition of a hash family in the natural manner.
	If $\F$ is a family of hash functions $\F : D \times \zo{r} \rightarrow R$ then we define for all $F \in \F$ and $\vec{x} \in D^k$:
	\begin{equation*}
		F(\vec{x}) = \left(F(x_1), F(x_2), \dots, F(x_k) \right)
	\end{equation*}
	
	The latter definition simply states that each input is handled separately as originally defined and the different outputs are outputted as a vector.
	
	\begin{definition} \label{Def: Symmetric distribution}
		We say a distribution $\dist$ over $\zo{k}$ is \emph{Symmetric} iff for all $\vec{x} \in \zo{k}$:
		\begin{equation*}
			\dist(\vec{x}) = \dist(\vec{x^c})
		\end{equation*}
		Where $\vec{x^c} = (1-x_1, 1-x_2, \dots, 1-x_k)$.
	\end{definition}
	
	\subsection{Fourier Expansion}
	
	The proofs of the main lemmas will rely heavily on the Fourier expansion of Boolean functions and the notion of bias, which will now be defined.
	This will only be a cursory introduction to the subject.
	For more details, we refer the reader to \cite{AnalysisOfBooleanFunctions}.
	In section \ref{section: reducing formula size} we will generalize these definitions to the non-Boolean case, but for the time being these suffice.
	
	\begin{definition}[Linear Boolean Functions]
		For every $\vec{x} \in \zo{n}$ and every subset $S \subseteq \left[n\right]$ we define the linear function $\chi_S(\vec{x})$ in the following manner:
		\begin{equation*}
			\chi_S(\vec{x}) = \bigoplus_{i \in S} x_i
		\end{equation*}	
		By convention: $\chi_\phi(\vec{x}) = 0$ for all $\vec{x}$.
	\end{definition}
	
	\begin{note}
		A linear test is defined by the subset $S$ of indices it includes in the sum.
		This parameter $S$ can be viewed either as a set $S \subseteq \left[n\right]$ or as an indicator vector $S \in \zo{n}$.
		Although the definition was given, for clarity's sake, in the first form, we will actually be using the second one more frequently.
		This will allow us to view the input to the function $\chi_S$ and the definition $S$ of the function itself as vectors over the same domain $\zo{n}$.
		This, in turn, yields the equation $\chi_S(\vec{x}) = \langle S, \vec{x} \rangle$.
	\end{note}
	
	The Fourier expansion of Boolean functions is written in a different basis from the usual one.
	The values $\{0,1\}$ are mapped to the values $\{1,-1\}$, respectively.
	One can verify that under this mapping, if $x,y$ are Boolean variables then $x \oplus y = x \cdot y$, where $\oplus$ is the usual XOR and $\cdot$ is multiplication over the reals.
	
	Functions defined over $D \rightarrow \{-1,1\}$ can be represented as vectors in $\{-1,1\}^{\abs{D}}$ where each coordinate stores the function's value for its respective input.
	It turns out that any Boolean function $f : \oo{n} \rightarrow \{-1,1\}$ can be written as a weighted sum of the linear functions.
	\begin{fact}[Fourier Expansion of a Boolean Function] \label{Fact: Fourier expansion}
		For all $f : \oo{n} \rightarrow \{-1,1\}$ there exists a function $\hat{f}: \oo{n} \rightarrow \left[-1,1\right]$ s.t. for all $\vec{x} \in \oo{n}$:
		\begin{equation*}
			f(\vec{x}) = \sum_{S \subseteq \left[n\right]} \hat{f}(S) \cdot \chi_S(\vec{x}) = \sum_{S \subseteq \left[n\right]} \hat{f}_S \cdot \prod_{i \in S} x_i
		\end{equation*}
		$\hat{f}$ is called the \emph{Fourier decomposition} of $f$, and its values are given by:
		\[\hat{f}(S) = \langle f, \chi_S \rangle \]
		In this last equation we used the vector representation of Boolean functions,
	\end{fact}
	
	\subsection{Bias}
	
	The following notion of bias of a distribution w.r.t to some linear test captures how much the test can help distinguish between that distribution and the uniform one.
	
	\begin{definition}[Bias with respect to a linear test] \label{Def: bias w.r.t linear test}
		For any distribution $\dist$ over domain $\oo{k}$ and any set $S \subseteq \left[k\right]$ s.t. $S \neq \phi$, we say $\dist$ is \emph{ $\epsilon$-biased w.r.t to test $\chi_S$} if
		\begin{equation*}
		\abs{\underset{\vec{x} \sim \dist}{\E} \left[ \chi_S(\vec{x}) \right]} \leq \epsilon
		\end{equation*}
	\end{definition}
	
	The next notion we define will be central to our arguments.
	It will allow us to aggregate bias bounds over many tests, which would prove to be extremely useful.
	
	\begin{definition} \label{t epsilon bias}
		A distribution $\dist$ over $\oo{k}$ is said to be \emph{$(t,\epsilon)$-biased} if for any subset of indices $S \in \left[k\right]$ s.t. $0 < \abs{S} \leq t$ : $\dist$ is $\epsilon$-biased w.r.t $\chi_S$.
	\end{definition}
	We say a distribution $\dist$ over $\oo{k}$ is $\epsilon$-biased if it is $(k, \epsilon)$-biased.
	
	One nice property of linear tests is that their output is completely uniform when given a uniformly random input.
	This is one of the incentives for this definition.
	The exception to this rule is the trivial linear test $\chi_\phi$, which is always constant.
	That is why the definition only includes linear tests $S$ of size $\abs{S} > 0$.
	
	
	\begin{definition} \label{definition: function family t-bias}
		A function family $\F : \oo{n} \times \oo{r} \rightarrow \{-1,1\}$ is said to be \emph{$(t, \epsilon)$-biased} if for any set of up to $t$ inputs $\vec{x}$: $\dist_{\F,\vec{x}}$ is $(t,\epsilon)$-biased.
	\end{definition}
	
	We note that $(t,\epsilon)$-independence is stronger than, and indeed implies, $(k, \epsilon)$-bias.
	The converse is not true.

\section{Basic Composition Lemmas} \label{section: composition lemmas}
	
	In this section we analyze the effects of the functions XOR and AND on $(t,\epsilon)$-biased distributions.
	We show that XOR reduces bias, while AND increases the independence parameter $t$.
	For simplicity's sake, we assume that we begin with a symmetric distribution.
	In later sections we will remove this restriction.
	
	We begin with the analysis of the XOR function.
	One of the properties of this function is that if we take the XOR of two independent distributions, then the bias of the resultant distribution w.r.t. any linear test is at most the minimum of the biases of the original distributions w.r.t. the same test.
	This property will be quite useful to us both in producing a generic construction and later on, when we attempt to generalize to a construction which recursively uses just one type of gate.
	
	\subsection{Effects of XOR}
	
	\begin{claim} \label{claim: Symmetry}
		Let $\dist$ be any symmetric probability distribution over $\oo{k}$ and $\dist'$ be any (not necessarily symmetric) probability distribution over the same domain.
		Then $\XOR(\dist,\dist')$ is a symmetric distribution.
	\end{claim}
	
	\begin{proof}
		
		Let $\vec{x} \in \Supp{\XOR(\dist,\dist')}$, and let $A$ be the set of all pairs of inputs $\left(\vec{x_1}, \vec{x_2}\right) \in \oo{k} \times \oo{k}$
		s.t. $\vec{x_1} \oplus \vec{x_2} = \vec{x}$. Then:
		
		\begin{align*}
		\Pr_{\vec{X} \sim \XOR(\dist,\dist')} \left[ \vec{X} = \vec{x} \right]
		&= \Pr_{\vec{X_1} \sim \dist, \vec{X_2} \sim \dist'} \left[ \vec{X_1} \oplus \vec{X_2} = \vec{x} \right] \\
		&= \Pr_{\vec{X_1} \sim \dist, \vec{X_2} \sim \dist'} \left[ \vec{X_1^c} \oplus \vec{X_2} = \vec{x} \right] \\
		&= \Pr_{\vec{X_1} \sim \dist, \vec{X_2} \sim \dist'} \left[ \vec{X_1} \oplus \vec{X_2} \oplus \vec{-1^k} = \vec{x} \right] \\
		&= \Pr_{\vec{X} \sim \XOR(\dist,\dist')} \left[ \vec{X} = \vec{x^c} \right]
		\end{align*}
		
		Where the second equality follows from the assumption that $\dist$ is symmetric.
		
	\end{proof}
	
	\begin{lemma}[XOR Bias reduction] \label{lemma: XOR bias reduction}
		Let $S \subseteq \left[ k \right]$ be a set of indices, $\dist$ be a distribution over $\oo{k}$ that is $\epsilon$-biased w.r.t $\chi_S$ and $\dist'$ be a distribution over $\oo{k}$ that is $\epsilon'$-biased w.r.t $\chi_S$
		Then: $\XOR\left(\dist,\dist\right)$ is $(\epsilon \cdot \epsilon')$-biased w.r.t. $\chi_S$.
	\end{lemma}
	
	\begin{proof}
		\begin{align*}
			\abs{\underset{\vec{X} \sim \XOR\left(\dist,\dist'\right)}{\E} \left[ \chi_S(\vec{X}) \right]}
			&= \abs{\underset{\vec{X} \sim \XOR\left(\dist,\dist'\right)}{\E} \left[ \prod_{i \in S} X_i \right]} \\
			&= \abs{\underset{\vec{Y} \sim \dist, \vec{Z} \sim \dist'}{\E} \left[ \prod_{i \in S} \left(Y_i \cdot Z_i \right) \right]} \\
			&= \abs{\underset{\vec{Y} \sim \dist, \vec{Z} \sim \dist'}{\E} \left[ \prod_{i \in S} Y_i \cdot \prod_{i \in S} Z_i  \right]} \\
			&= \abs{\underset{\vec{Y} \sim \dist}{\E} \left[ \prod_{i \in S} Y_i \right] \cdot \underset{\vec{Z} \sim \dist'}{\E} \left[ \prod_{i \in S} Z_i \right]} \\
			&= \abs{\underset{\vec{Y} \sim \dist}{\E} \left[ \prod_{i \in S} Y_i \right]} \cdot \abs{\underset{\vec{Z} \sim \dist}{\E} \left[ \prod_{i \in S} Z_i \right]} \\
			&< \epsilon \cdot \epsilon'
		\end{align*}
		
		Where the 4-th equality follows from the independence of $\vec{Y}$ and $\vec{Z}$ and the last inequality follows from the assumption on $\dist$.
		
	\end{proof}
	
	This lemma has two immediate and useful corollaries:
	
	\begin{corollary} \label{corollary: XOR bias}
		If $\dist$ is $\epsilon$ biased w.r.t. $\chi_S$ then:
		\begin{enumerate}
			\item $\XOR(\dist, \dist)$ is $\epsilon^2$-biased w.r.t. $\chi_S$.
			\item For any distribution $\dist'$ defined over the same domain: $\XOR(\dist, \dist')$ is $\epsilon$-biased w.r.t. $\chi_S$.
		\end{enumerate}
	\end{corollary}
	
	\subsection{Effects of AND}
	
	In this section we prove the following proposition.
	\begin{proposition} \label{proposition: simple ANDXOR bias}
		If $\dist$ is a symmetric, $(t, \epsilon)$-biased probability distribution then:
		$\XOR(\dist, \AND(\dist,\dist))$ is symmetric and $(2t, \frac{1}{2} + 2^{-t} + \epsilon)$-biased,
		where the samples from $\dist$ are independent.
	\end{proposition}
	
	The next lemma establishes the simplest form of our main idea, and can be treated as a proof-of-concept.
	It basically states that AND-ing together two samples from a $(t,\epsilon)$-biased distribution, for $t \geq 2$ and a suitably small value $\epsilon$, forms a $(2t,c)$-biased distribution for some not-too-large constant $c$.
	
	\begin{lemma} \label{lemma: AND test enhancement}
		Let $t \in \mathbb{N}$ and let $\dist$ be a symmetric, $(t,\epsilon)$-biased probability distribution over $\oo{k}$.
		Then for any $S \subseteq \left[k\right]$ s.t. $0 < \abs{S} \leq 2t$:
		$\AND(\dist,\dist)$ is $\frac{1}{2} + \epsilon + 2^{-\min(t, \abs{S})}$-biased w.r.t $\chi_S$.
	\end{lemma}
	
	Note that this result gives only the trivial bound on the bias of tests $S$ of size 1.
	We can overcome this by XOR-ing with an additional copy from the original distribution.
	This also has the effect of making the final distribution symmetric once more.
	With this in mind, we may now prove the proposition.
	
	\begin{proof}[Proof of proposition \ref{proposition: simple ANDXOR bias}]
		Let $S \subseteq \left[k\right]$ s.t. $0 < \abs{S} \leq 2t$.
		If $\abs{S} \geq t$ then the we get the required bias from lemma \ref{lemma: AND test enhancement}.
		If $\abs{S} < t$, then we have two options:
		\begin{enumerate}
			\item If $\epsilon > \frac{1}{2}$ then the bound specified by the proposition is meaningless, since it bounds a probability by a constant larger than $1$, which is a tautology.
			\item If $\epsilon < \frac{1}{2}$ then the bias of w.r.t $S$ must be at most $\frac{1}{2}$ by corollary \ref{corollary: XOR bias} and the fact that $\dist$ is $(t, \epsilon)$-biased.
		\end{enumerate}
		Finally, by claim \ref{claim: Symmetry}: since $\dist$ is symmetric: so is $XOR(\dist, AND(\dist, \dist))$.
	\end{proof}
	
	We now turn to proving the lemma.
	\begin{proof}[Proof of lemma \ref{lemma: AND test enhancement}]
		
		The intuition behind the lemma is the following.
		Let $\vec{x}, \vec{y}$ be independent samples from $\dist$.
		Note that whenever a bit $x_i = 1$: then $\AND(x_i, y_i) = 1$ as well.
		This means that this bit does not change the result of the test $\chi_S$.
		
		Let $\wt{\vec{x_S}}$ denote the weight of $\vec{x_S}$, which is the number of bits in $\vec{x_S}$ which are equal to $-1$.
		Then if $1 \leq \wt{ \vec{x_S}} \leq t$, then $\chi_S(\AND(\vec{x}, \vec{y}))$ is a linear test of size between $1$ and $t$ over $\vec{y}$, and can be bounded by $\epsilon$.
		We now show this more rigorously and then proceed to bound the probability that $\wt{\vec{x}}$ is outside of the interval $\left[1,t\right]$.
		
		Let $P_1,P_2 \subseteq \{-1,1\}^k$ s.t. $P_2 = \{ \vec{x} \in \oo{k} \, \vert \, \vec{x^c} \in P_1 \}$, $P_1 \cap P_2 = \phi$ and for all $\vec{x} \in P_1$ it holds that $\wt{\vec{x_S}} \leq t$. Such sets can be constructed by greedily choosing pairs of complementing vectors from $\{-1,1\}^k$ and placing each of them in the correct set (or placing them randomly when both options are valid).
		
		Since $\dist$ is symmetric:
		\begin{equation*}
			\Pr_{\vec{X} \sim \dist} \left[ \vec{X} \in P_1 \right] = \Pr_{\vec{X} \sim \dist} \left[ \vec{X} \in P_2 \right]
		\end{equation*}
		Therefore:
		\begin{equation*}
		 \Pr_{\vec{X} \sim \dist} \left[ \wt{\vec{X_S}} \leq t \right] \geq \frac{1}{2}
		\end{equation*}
		
		Thus:
		\begin{equation} \label{equation: 2t half way}
		\begin{alignedat}{3}
			\abs{\underset{\vec{X} \sim \AND(\dist,\dist)}{\E} \left[ \prod_{i \in S}{X_i} \right]}
			&= && \abs{ \underset{\vec{Y},\vec{Z} \sim \dist}{\E}
				\left[
					\prod_{i \in S} Y_i \wedge Z_i
				\right]} \\
			&\leq && \abs{ \Pr_{\vec{Z} \sim \dist} \left[ \wt{\vec{X_S}} > t \right]
					\cdot \underset{\vec{Y},\vec{Z} \sim \dist}{\E}
						\left[
							\prod_{i \in S} Y_i \wedge Z_i \middle| \vec{Z} \in P_2
						\right]} \\
			& && + \abs{ \Pr_{\vec{Z} \sim \dist} \left[ 1 \leq \wt{\vec{Z}} \leq t \right]
					\cdot \underset{\vec{Y},\vec{Z} \sim \dist}{\E}
						\left[
							\prod_{i \in S} Y_i \wedge Z_i \middle| 1 \leq \wt{\vec{Z}} \leq t
						\right] }\\
			& && + \abs{ \Pr_{\vec{Z} \sim \dist} \left[ \wt{\vec{Z}} = 0 \right]
					\cdot \underset{\vec{Y},\vec{Z} \sim \dist}{\E}
						\left[
							\prod_{i \in S} Y_i \wedge Z_ii \middle| \wt{\vec{Z}} = 0
						\right] }\\
			&\leq && \frac{1}{2} + \frac{1}{2}\epsilon + \Pr_{\vec{Z} \sim \dist} \left[ \wt{\vec{Z}} = 0 \right]
		\end{alignedat}
		\end{equation}
		
		Where the final inequality is by $\vec{Y}$ being $\epsilon$-biased w.r.t. tests of size between $1$ and $t$, and by the trivial bound on the bias in the other cases.
		
		We now need a bound on the probability that a random sample from $\dist$, restricted to $S$ has all of its components equal to $0$.
		The following claim, which will be subsequently proved, establishes the required bound.
		
		\begin{claim}  \label{claim: Zero vector probability}
			Let $\dist$ be a $(t,\epsilon)$-biased probability distribution over $\oo{k}$ and $S \in \oo{k}$. Then:
			\begin{equation*}
				\Pr_{\vec{X} \sim \dist} \left[ \vec{X_S} = \vec{1^{\abs{S}}} \right]
				\leq 2^{-\min(t, \abs{S})} + \frac{1}{2}\epsilon
			\end{equation*}
		\end{claim}
		
		Assigning the values from this claim into equation \ref{equation: 2t half way} yields the lemma.
		
	\end{proof}
	
	\begin{proof}[Proof of Claim \ref{claim: Zero vector probability}]
		
		If $\abs{S} > t$, then we can restrict $\vec{X}$ to some subset $S' \subseteq S$ such that $\abs{S'} = t$.
		We then need only prove that $\Pr_{\vec{X} \sim \dist} \left[ \vec{X_{S'}} = \vec{1^{\abs{S'}}} \right]
		\leq 2^{-t} + \frac{1}{2}\epsilon$.
		Instead, and without loss of generality, we will assume that $\abs{S} \leq t$ and prove the originally stated inequality.
		
		Let $\OR : \oo{\abs{S}} \rightarrow \{-1,1\}$ be the binary OR function on $\abs{S}$ bits and let $\widehat{\OR} : \oo{S} \rightarrow \left[-1,1\right]$ denote its Fourier decomposition.
		Notice that $\vec{X_S} = \vec{1^t}$ iff $\OR_{i \in S} X_i = 1$.
		Then:
		
		\begin{equation*}
		\begin{alignedat}{3}
			\Biggr\vert \underset{\vec{X} \sim \dist}{\E} \left[ \OR_{i \in S} X_i \right]
				&- &&\underset{\vec{X} \sim \mathcal{U}}{\E} \left[ \OR_{i \in S} X_i \right] \Biggr\vert = \\
			&= &&\abs{ \underset{\vec{X} \sim \dist}{\E}
				\left[
					\sum_{M \subseteq S} \left( \widehat{\OR}(M) \cdot \prod_{i \in M} X_i \right)
				\right]
				- \underset{\vec{X} \sim \mathcal{U}}{\E}
				\left[
					\sum_{M \subseteq S} \left( \widehat{\OR}(M) \cdot \prod_{i \in M} X_i \right)
				\right] } \\
			&= && \Bigg\vert \widehat{\OR}(\phi) + \sum_{\phi \neq M \subseteq S}
				\left(
					\widehat{\OR}(M) \cdot \underset{\vec{X} \sim \dist}{\E} \left[ \prod_{i \in M} X_i \right]
				\right) \\
			& && - \widehat{\OR}(\phi) + \sum_{\phi \neq M \subseteq S}
				\left(
				\widehat{\OR}(M) \cdot \underset{\vec{X} \sim \mathcal{U}}{\E} \left[ \prod_{i \in M} X_i \right]
				\right) \Bigg\vert \\
			&\leq && \epsilon \abs{ \sum_{\phi \neq M \subseteq S} \widehat{\OR}(M)}\\
			&\leq && \epsilon
		\end{alignedat}
		\end{equation*}
		
		In which the penultimate inequality stems from our assumption that $\dist$ is $(t,\epsilon)$-biased, the fact that $\abs{S} \leq t$ and the fact that the uniform distribution is unbiased w.r.t. all (non-trivial) linear tests.
		The final inequality stems from the following well-known fact about the Fourier expansion of the OR functions:
		
		\begin{fact}
			Let $\hat{V} : \{0,1\}^{\left[ t \right]} \rightarrow \left[-1,1\right]$ be the Fourier coefficients of the OR function on $t$ bits. Then:
			\begin{equation*}
			0 \leq \sum_{\phi \neq S \subseteq \left[ t \right]} \hat{M}(S) \leq 1
			\end{equation*}
		\end{fact}
		
		Now, notice that
		\begin{equation*}
		\underset{\vec{X} \sim \mathcal{U}}{\E} \left[ \bigvee_{i \in S} X_i \right]
		= 1 \cdot 2^{-\abs{S}} - 1 \cdot (1 - 2^{-\abs{S}}) = -1 + 2^{1-\abs{S}}
		\end{equation*}
		
		It therefore follows that:
		
		\begin{align*}
		2 \cdot \Pr_{\vec{X} \sim \dist} \left[ \bigvee_{i \in S} X_i = 1 \right] - 1
		&= \Pr_{\vec{X} \sim \dist} \left[ \bigvee_{i \in S} X_i = 1 \right]
			- \Pr_{\vec{X} \sim \dist} \left[ \bigvee_{i \in S} X_i = -1 \right] \\
		&= \underset{\vec{X} \sim \dist}{\E} \left[ \bigvee_{i \in S} X_i \right] \\
		&\leq -1 + 2^{1-\abs{S}} + \epsilon
		\end{align*}
		
		Which, in turn, means that:
		\begin{align*}
		\Pr_{\vec{X} \sim \dist} \left[ \vec{X_S} = \vec{1^{\abs{S}}} \right]
		&= \Pr_{\vec{X} \sim \dist} \left[ \bigvee_{i \in S} X_i = 1 \right] \\
		&\leq \frac{1}{2} \left( 2^{1-\abs{S}} + \epsilon \right) \\
		&= 2^{-\abs{S}} + \frac{1}{2}\epsilon \\
		&= 2^{-\min\left(t, \abs{S}\right)} + \frac{1}{2}\epsilon
		\end{align*}
		
	\end{proof}
	

\section{Basic Construction} \label{section: generic constructions}
	
	In this section we provide explicit constructions which will serve as a basis for our more efficient versions.
	We first show how to apply the lemmas of the previous section to increasing the bounded independence of any hash function family which takes bit strings and maps them into single bits.
	We then show how these results may be applied in creating a hash function family which closely resembles the construction of \cite{Valiant}.
	
	\subsection{General transformation} \label{section: general binary transformation}
	
	In this section we present our first theorem, which will put the lemmas and corollaries from the previous section into a useful framework.
	We make no attempts at this point to optimize the construction in any way.
	In particular, we only deal with hash function families which are defined over bit strings and have single output bits.
	More general and efficient constructions are deferred to later sections.
	
	Note also that in this section we use the more readily-familiar $\{0,1\}$-basis since all calculations using Fourier decomposition are contained in the previous section.
	
	\begin{theorem} \label{theorem: simplest hash enhancement}
		Let $\F : \zo{n} \times \zo{r} \rightarrow \{0,1\}$ be a family of $(k_0, \epsilon_0)$-wise independent hash functions, with $k_0 \geq 2$ and $\epsilon_0 < 1$. Then for all $k$ and all $\epsilon > 0$: it is possible to (explicitly and efficiently) construct a family $\F : \zo{n} \times \zo{r} \rightarrow \{0,1\}$ that is $(k, \epsilon)$-wise independent with $r = \Poly{k, \log \frac{1}{\epsilon}, \frac{1}{\log \frac{1}{\epsilon_0}}} \cdot r_0$ using only black-box access to $\F_0$.
	\end{theorem}
	
	\begin{proof}
		We construct the new hash function in three stages:
		\begin{enumerate}
			
			\item Reduce the initial bias enough to use corollary \ref{proposition: simple ANDXOR bias}, by repeatedly XOR-ing samples from $\F_0$.
			
			\item Repeat the following two sub-steps until the independence parameter reaches $k$:
			\begin{enumerate}
				\item AND two samples from the current hash function family in order to increase the independence parameter.
				\item Apply XOR on the result sufficiently many times to reduce the bias back to a useful value.
			\end{enumerate}
			
			\item Successively apply XOR steps in order to reduce the bias enough to imply $\epsilon$-wise independence.
			
		\end{enumerate}
		This process is described in detail in figure \ref{figure: Algorithm}.
		
		\begin{figure}[H]
			\removelatexerror
			\begin{mdframed}
				
			\begin{description}
				\item[Input:] $(k_0,\epsilon_0)$-biased hash family $\F_0 : \zo{n} \times \zo{r_0} \rightarrow \{0,1\}$.
			\end{description}
			
			\begin{description}
				\item[Output:] $(k,\epsilon)$-biased hash family $\F : \zo{n} \times \zo{r} \rightarrow \{0,1\}$.
			\end{description}
			
			\begin{description}
				\item[Phase 1:] \textbf{Initial bias reduction}
				
				\begin{itemize}
					\item Repeat until $\epsilon_i \leq \frac{1}{8}$:
					\begin{itemize}
						\item $\F_i \leftarrow \XOR(\F_{i-1}, \F_{i-1})$
					\end{itemize}
				\end{itemize}
			\end{description}
			
			\begin{description}
				\item[Phase 2:] \textbf{increasing the independence parameter}
				
				\begin{itemize}
					\item Repeat until $k_i \geq k$:
					\begin{itemize}
						\item $\F_i \leftarrow \XOR(\F_{i-1}, \AND(\F_{i-1}, \F_{i-1}))$
						\item Repeat until  $\epsilon_i \leq \frac{1}{8}$:
						\begin{itemize}
							\item $\F_i \leftarrow \XOR(\F_{i-1}, \F_{i-1})$
						\end{itemize}
					\end{itemize}
				\end{itemize}
			\end{description}
			
			\begin{description}
				\item[Phase 3:] \textbf{Final bias reduction}
				
				\begin{itemize}
					\item Repeat until $\epsilon_i \leq 2^{-k/2}\epsilon$:
					\begin{itemize}
						\item $\F_i \leftarrow \XOR(\F_{i-1}, \F_{i-1})$
					\end{itemize}
				\end{itemize}
			\end{description}
			
			\end{mdframed}
			\caption{Basic algorithm for increasing bounded independence} \label{figure: Algorithm}
		\end{figure}
		
		First note that if the algorithm terminates then its output is $(k, 2^{-k/2} \epsilon)$-biased.
		By lemma \ref{lemma: diaconis bias} this makes it also $(k,\epsilon)$-wise independent, as required.
		Now, by proposition \ref{proposition: simple ANDXOR bias} and corollary \ref{corollary: XOR bias}: the each of the loops in the algorithm always terminates.
		Therefore: the algorithm is correct.
		We now analyze the amount of randomness required by it.
		
		In the first stage of the construction we reduce the bias down to $\frac{1}{8}$.
		By corollary \ref{corollary: XOR bias}, after $s_1$ iterations of XOR-ing, the new hash function family $\F_{s_1}$ is $(k_0, \epsilon_{s_1})$-biased for $\epsilon_{s_1} = \epsilon_0^{2^{s_1}}$.
		We therefore require $s_1 = \max (0, \log \log 8 - \log \log \frac{1}{\epsilon_0})$ such steps, each of which requires two samples from the previous step's distribution.
				
		In the second stage we sample 3 times from our new hash function family and then combine these samples as suggested in \ref{proposition: simple ANDXOR bias}: $\XOR(f_1, \AND(f_2, f_3))$.
		This yields a $(2k_0, \frac{7}{8})$-biased hash function.
		Applying 4 rounds of XOR results in a $(2k_0, \frac{1}{8})$-biased hash function which can be now be used in the same process to produce a $(4k_0, \frac{1}{8})$-biased family and so forth.
		The number of steps required in this stage is $s2 = \log k - \log k_0$, where each step needs $3 \cdot 2^4 = 48$ samples from the previous step's distribution.
		
		The final stage is similar to the first one and requires $s_3 = \max(0, \log \log \frac{2^{k/2}}{\epsilon} - \log \log 8)$ steps.
		
		We are now ready to calculate the amount of randomness required by this construction.
		We assume for simplicity that $\epsilon_0 > \frac{1}{8}$ and that $\epsilon < \frac{1}{8}$.
		
		\begin{align*}
			r'
			&= 3^{s_2} \cdot 2^{s_1 + 4s_2 + s_3} \cdot r \\
			&= 3^{\log k - \log k_0}
				\cdot 2^{\log \log 8 - \log \log \frac{1}{\epsilon_0}
						+ 4\log k - 4\log k_0
						+ \log\log \left( 2^{k/2} \cdot \frac{1}{\epsilon} \right) - \log\log \frac{1}{8} } 
				\cdot r\\
			&= \left( \frac{k}{k_0} \right)^{\log_2 3} \cdot \left( \frac{k}{k_0} \right)^4
				\cdot \frac{1}{\log \frac{1}{\epsilon_0}} \cdot \left[ \frac{1}{2}k + \log \frac{1}{\epsilon} \right]
				\cdot r \\
			&= o\left( \frac{k^6 r}{\log \frac{1}{\epsilon_0}} \cdot \left(k + \log \frac{1}{\epsilon}\right) \right)
		\end{align*}
		
	\end{proof}
	
	The construction presented above can be viewed as a complete, balanced tree that has the property that all nodes of equal depth contain the same type of gate.
	This simple structure can be made even simpler at a further cost to the efficiency of the construction.
	
	Specifically, look at the following gate:
	\begin{equation*}
		\mathrm{Universal}(x_1, x_2, x_3, x_4) = (x_1 \wedge x_2) \oplus x_3 \oplus x_4
	\end{equation*}
	
	\begin{claim} \label{claim: universal gate}
		If all the gates in the construction of theorem \ref{theorem: simplest hash enhancement} are replaced with the gate $\mathrm{Universal}$ then the final output is still $(k', \epsilon')$-wise-independent.
	\end{claim}
	
	\begin{proof}
		By corollary \ref{corollary: XOR bias}: $\mathrm{Universal}(x_1, x_2, x_3, x_4)$ has bias not larger than that of $(x_1 \wedge x_2) \oplus x_3$ or that of $(x_3 \oplus x_4)$.
		Since these are the only types of gates used in the original construction, the claim follows.
	\end{proof}
	
	\subsection{A New Family of Hash Functions} \label{section: Simple circuit construction}
		
	In \cite{Valiant}, it was shown that a polynomial-size monotone formula can be used to calculate the majority function over $n$ bits.
	This was done by sampling these bits independently at random some $m>n$ times and calculating a formula on these $m$ bits.
	At this point a probabilistic argument was used to prove that there exists a random seed such that the formula computes the majority of the inputs on \emph{all} possible inputs.
	
	We will use the same random-sampling technique to generate a "seed" $(2, \epsilon)$-independent hash function family.
	Our generic construction from theorem \ref{theorem: simplest hash enhancement} can then be applied on this "seed" to achieve $(k, \epsilon)$-independence for any $k = \Poly{n}$ and $\epsilon \geq \Omega(2^{-2^{\Poly{n}}})$.
	
	Explicitly, let $\zo{n}$ be the domain of the hash function family, and let $k$ and $\epsilon$ be some target parameters.
	We define the following hash function family: 
		$\F : \zo{n} \times \left[2n+2\right] \rightarrow \{0,1\}$ as:
	\begin{equation*}
		\F(\vec{x}, \rho) =
		\begin{cases}
			x_i & \rho \leq n \\
			1-x_i & n + 1 \leq \rho \leq 2n \\
			0 & \rho = 2n+1 \\
			1 & \rho = 2n+2
		\end{cases}
	\end{equation*}
	Where sampling a hash function from the family is done by sampling
		$\rho \overset{R}{\leftarrow} \left[2n + 2\right]$.
	
	Note that any function from this family can be implemented as a formula with at most 2 inputs (one of which is the constant $1$) and at most a single XOR gate.
	Also, the number of bits required to sample from this hash function family is $r = \log(2n+2) = O(\log n)$.
	
	\begin{claim} \label{claim: bias of basic Valiant hash}
		$\F$ has the following two properties:
		\begin{enumerate}
			\item For all $k>0$ and all $\vec{x} \in \zo{k}$: $\dist_{\F,\vec{x}}$ is symmetric.
			\item $\F$ is $(2,1 - \frac{2}{n+1})$-biased.
		\end{enumerate}
	\end{claim}
	
	Combining claim \ref{claim: bias of basic Valiant hash} with theorem \ref{theorem: simplest hash enhancement} yields a family of $(k, \epsilon)$-wise independent hash functions. This construction has a structure similar to the formula for the majority function presented in \cite{Valiant}.
	
	\begin{proof}
		
		Let $\rho \in \left[n+1\right]$. 
		Then for all $\vec{x}$:
		\begin{equation*}
			\F(\vec{x}, \rho) = \F(\vec{x}, \rho + n + 1)^c
		\end{equation*}
		This concludes the proof of item 1.
		
		\begin{claim} \label{claim: symmetry yields (1,0)-bias}
			If a distribution $\dist$ over $\oo{k}$ is symmetric, then it is also $(1,0)$-biased.
		\end{claim}
		Using this claim, which will be proved shortly, we immediately have that $\F$ is $(1,0)$-biased.
		
		Now let $\vec{X} = \left(\vec{y}, \vec{z}\right) \in (\zo{n})^2$ be a set of two distinct inputs to the function.
		The pairwise bias of $\F$ is:
		\begin{equation*}
			\abs{\Pr_{\rho \rand \left[ 2n+2 \right]} \left[ \F(\vec{X}, \rho)_1 \oplus \F(\vec{X}, \rho)_2 = 1 \right]
			- \Pr_{\rho \rand \left[ 2n+2 \right]} \left[ \F(\vec{X}, \rho)_1 \oplus \F(\vec{X}, \rho)_2 = 0 \right]}
		\end{equation*}
		Since $\vec{y} \neq \vec{z}$: there exists at least one index $i \in \left[n\right]$ s.t. $y_i \neq z_i$.
		Without loss of generality, let us assume that $i=1$.
		Therefore, if $\rho \in \{1, n+2\}$ then $\F(\vec{X}, \rho)_1 \oplus \F(\vec{X}, \rho)_2 = 1$.
		But by the function definition, if $\rho \in \{2n+1, 2n+2\}$ then $\F(\vec{X}, \rho)_1 \oplus \F(\vec{X}, \rho)_2 = 1$.
		Therefore, for $b \in \{0,1\}$:
		\begin{equation*}
			\Pr_{\rho \rand \left[ 2n+2 \right]} \left[ \F(\vec{X}, \rho)_1 \oplus \F(\vec{X}, \rho)_2 = b \right] \leq 1 - \frac{1}{n+1}
		\end{equation*}
		Which means that:
		\begin{equation*}
		\abs{\Pr_{\rho \rand \left[ 2n+2 \right]} \left[ \F(\vec{X}, \rho)_1 \oplus \F(\vec{X}, \rho)_2 = 1 \right]
			- \Pr_{\rho \rand \left[ 2n+2 \right]} \left[ \F(\vec{X}, \rho)_1 \oplus \F(\vec{X}, \rho)_2 = 0 \right]}
		\leq 1 - \frac{2}{n+1}
		\end{equation*}
		
	\end{proof}
	
	\begin{note}
		Claim \ref{claim: universal gate} together with these last results imply that replacing the gate in the construction of \cite{Valiant} with the $\mathrm{Universal}$ gate from the previous section results in an almost $k$-wise independent hash function, where the parameters $k$ and $\epsilon$ are determined by the number of samples taken from the input.
	\end{note}
	
	We now prove claim \ref{claim: symmetry yields (1,0)-bias}.
	In fact, we will prove a stronger claim that immediately implies it.
	
	\begin{claim} \label{claim: odd bias}
		If a distribution $\dist$ over $\oo{k}$ is symmetric, then it is also 0-biased with respect to all linear tests of odd size.
	\end{claim}
	
	\begin{proof}
		Let $S \in \oo{k}$ s.t. $\wt{S}$ is odd; let $A = \{\vec{y} \in \Supp{\dist} \, \vert \, \chi_S(\vec{y} = 1) \}$ and $\bar{A} = \Supp{\dist} \setminus A$. Then:
		
		\begin{align*}
		\Pr_{\vec{Y} \sim \dist} \left[ \chi_S(\vec{Y}) = 1 \right]
		&= \sum_{\vec{y} \in A} \Pr_{\vec{Y} \sim \dist} \left[ \vec{Y} = \vec{y} \right]\\
		&= \sum_{\vec{y} \in A} \Pr_{\vec{Y} \sim \dist} \left[ \vec{Y} = \vec{y^c} \right]\\
		&= \sum_{\vec{y} \in \bar{A}} \Pr_{\vec{Y} \sim \dist} \left[ \vec{Y} = \vec{y} \right]\\
		&= \Pr_{\vec{Y} \sim \dist} \left[ \chi_S(\vec{Y}) = -1 \right]
		\end{align*}
		Where we used symmetry in the second equality and the fact that $\abs{S}$ is odd in the third.
		
	\end{proof}

\section{Reducing Key Size} \label{section: reducing randomness}
	
	In this section we are interested in reducing the key size of our generic constructions.
	In particular, we would like it to be linearly (instead of polynomially) dependent on $k$.
	To this end, we generalize our results to arbitrary input and output sizes.
	
	We will therefore start using the convention that hash function families are of the form $\F: D \times \zo{r} \rightarrow \Z_q$ for arbitrary domain $D$ and $q \in \mathbb{N}$.
	Note that although the outputs of hash functions are sometimes not in any group $\Z_q$, they can always be embedded in such a group, so long as we do not care how our transformations affect their original domain.
	Therefore, this definition is without loss of generality.
	
	It will later turn out to be the case that larger outputs allow us to rapidly increase the independence parameter $k$ without using more samples from the distribution, which will be very useful.
	This is because there are many more functions over larger alphabets than over small ones.
	This property will enable us to have more fine-grained control over the rate at which the independence parameter increases.
	Details shortly follow.
	
	In order to claim anything meaningful about functions of this sort we need to generalize the notion of bias to functions over groups $\Z_q$, where $\Z_q = \{0, 1, \dots, q-1\}$ with addition modulo $q$ as the group operation.
	We also need to generalize the Fourier decomposition of functions to work on ones that have outputs in $\Z_q$.
	
	\subsection{A Short Primer On Fourier Analysis of Functions Over $\Z_q$}
	
	We roughly follow the exposition given in \cite{BabaiFourierNotes}.
	
	Let $\Z_q$ be the group $\{0, 1, \dots, q-1\}$ with addition modulo $q$ as the group operation, and let $G = \Z_q^k$ be the vector space of dimension $k$ over $\Z_q$.
	Note that $G$ with vector summation modulo $q$ is also a group.
	
	We denote by $\C$ the field of complex numbers.
	We use $\T$ to denote the group of complex numbers of unit length with multiplication over $\C$ as the group operation.
	\begin{definition}
		The \emph{characters} of a group $G$ are all homomorphisms $\chi : G \rightarrow \T$.
		The set of characters of $G$ is denoted by $\hat{G}$.
	\end{definition}
	
	\begin{fact}
		The set of characters of $G$ forms a group under the following definition of group product:
		\begin{equation*}
			\chi_1 \ast \chi_2(x) = \chi_1(x) \cdot \chi_2(x)
		\end{equation*}
		In which $\cdot$ is the usual multiplication over the complex field.
	\end{fact}
	
	It turns out that $\hat{G}$ is isomorphic to $G$.
	Specifically, with every element $\vec{s} \in G = \Z_q^k$ we identify the following element $\chi_{\vec{s}} : G \rightarrow \T$:
	\begin{equation*}
		\chi_{\vec{s}}(\vec{x}) = e^{ \frac{2 \pi i}{k} \langle \vec{s}, \vec{x} \rangle }
	\end{equation*}
	Where $i = \sqrt{-1}$.
	One can see that $\chi_S$ is the exponentiation of a linear test on $\vec{x}$ that is defined by $\vec{s}$.
	
	The set of functions $f : G \rightarrow \C$ forms a $\abs{G} = \abs{\Z_q^k} = q^k$-dimensional vector space $\C^G$.
	\begin{definition}
		The \emph{inner product} of two functions $f_1, f_2 : G \rightarrow \C$ is defined as:
		\begin{equation*}
			\langle f_1, f_2 \rangle
			= \frac{1}{\abs{G}} \sum_{\vec{s} \in G} f_1(\vec{s}) \cdot f_2(\vec{s})
			= q^{-k} \sum_{\vec{s} \in G} f_1(\vec{s}) \cdot f_2(\vec{s})
		\end{equation*}
	\end{definition}
	
	\begin{fact}
		The set of characters $\{\chi_{\vec{s}} \mid \vec{s} \in G\}$ of a group $G$ forms an orthonormal basis of $\C^G$.
		That is, for all pairs $\vec{s_1}, \vec{s_2} \in G$:
		\begin{equation*}
			\langle \chi_{\vec{s_1}}, \chi_{\vec{s_2}} \rangle =
			\begin{cases}
				1 & \vec{s_1} = \vec{s_2} \\
				0 & \vec{s_1} \neq \vec{s_2} \\
			\end{cases}
		\end{equation*}
	\end{fact}
	
	Any function $f : G \rightarrow \C$ can be decomposed in the following way:
	\begin{equation*}
		f = \sum_{\vec{s} \in G} \hat{f}(\vec{s}) \cdot \chi_{\vec{s}}
	\end{equation*}
	That is to say, $f$ can be written as a linear combination of its characters.
	
	The function $\hat{f}$ is called the Fourier decomposition of $f$, and its values are given by:
	\begin{equation*}
		\hat{f}(\vec{s}) = \langle f, \chi_{\vec{s}} \rangle
	\end{equation*}
	
	Note that a distribution $\dist$ over $G$ is itself a function $\dist : G \rightarrow \C$, and can be decomposed in the same way.
	We now generalize the notion of bias as defined previously for the binary case.
	
	\begin{definition}
		The \emph{bias} of a distribution $\dist$ with respect to a linear test $\vec{s} \in G$ is the quantity:
		\begin{equation*}
			\bias{\dist}{\vec{s}} = \abs{ \underset{\vec{X} \sim \dist}{\E} \left[  \chi_{\vec{s}}(\vec{X}) \right] }
		\end{equation*}
	\end{definition}
	
	\begin{note}
		The definition given here is not the only possible one.
		Other, stronger generalizations of bias exist (see for example in \cite{GeneralBias}).
		However, in our context the notion defined here fits best, and in most cases replaces the special case seamlessly.
	\end{note}
	
	\begin{definition}
		The \emph{support} of a vector $\vec{s} \in \Z_q^k$ is defined to be the set of components of $\vec{s}$ that have non-zero value.
		It is denoted by $\Supp{\vec{s}}$.
	\end{definition}
	
	\begin{definition}
		The \emph{Hamming weight} of a vector $\vec{s} \in \Z_q^k$ is the number of non-zero components of $\vec{s}$.
		It is denoted by $\wt{\vec{s}} = \abs{\Supp{\vec{s}}}$.
	\end{definition}
	
	Note that the weight of a vector $\vec{s}$ defines the number of components of $\vec{x} \in G = \Z_q^k$ which the linear test $\chi_{\vec{s}}(\vec{x})$ depends on.
	
	\begin{definition}
		A distribution $\dist$ over $\Z_q^l$ is said to be $(t, \epsilon)$-biased if it is $\epsilon$-biased w.r.t. any linear test $\chi_{\vec{s}}$ with $0 < \wt{\vec{s}} \leq t$.
	\end{definition}
	As in the binary case, we say a distribution over $\Z_q^k$ is $\epsilon$-biased if it is $(k, \epsilon)$-biased.
	
	We are now ready to state the general version of the bias-to-independence bound which will see heavy use in the coming sections.
	\begin{lemma} \label{lemma: diaconis bias general case}
		Let $\F : D \rightarrow \Z_q $ be a $(k, \epsilon)$-biased hash function family for some domain $D$
		Then $\F$ is $(k, q^{k/2} \epsilon)$-wise independent.
	\end{lemma}
	We postpone the proof of this lemma to appendix \ref{appendix: diaconis bias proof}.
	
	\subsection{Generalized Combination Lemmas}
	
	\subsubsection{Reducing Bias}
	
	We are now ready to present a general framework for enhancing the bounded independence of distributions over general alphabets $\Z_q^k$.
	We begin with generalizing lemma \ref{lemma: XOR bias reduction}.
	For this purpose we denote by ADD the operation of addition modulo $q$ defined both over $\Z_q$ and over $\Z_q^k$.
	
	\begin{lemma} \label{lemma: ADD bias reduction}
		Let $\dist_1, \dist_2$ be a probability distributions over $\Z_q^k$ and let $\chi_{\vec{s}}$ be a linear test defined over the same domain.
		Then if $\dist_i$ is $\epsilon_i$-biased w.r.t $\chi_{\vec{s}}$ for $i \in \{1,2\}$ then $\dist = \ADD(\dist_1, \dist_2)$ is $\epsilon_1 \cdot \epsilon_2$-biased w.r.t. $\chi_{\vec{s}}$.
	\end{lemma}
	
	\begin{proof}
		The proof is basically identical to the one in the binary case.
		\begin{align*}
			\bias{\dist}{\vec{s}}
			&= \abs{ \underset{\vec{X} \sim \dist}{\E} \left[ \chi_{\vec{s}}(\vec{X}) \right] } \\
			&= \abs{ \underset{\vec{X_1} \sim \dist_1, \vec{X_2} \sim \dist_2}{\E}
				\left[
					\chi_{\vec{s}}(\vec{X_1} + \vec{X_2})
				\right] } \\
			&= \abs{ \underset{\vec{X_1} \sim \dist_1, \vec{X_2} \sim \dist_2}{\E}
				\left[
					\chi_{\vec{s}}(\vec{X_1}) \cdot \chi_{\vec{s}}(\vec{X_2})
				\right] } \\
			&= \abs{
					\underset{\vec{X_1} \sim \dist_1}{\E} \left[ \chi_{\vec{s}}(\vec{X_1}) \right]
					\cdot \underset{\vec{X_2} \sim \dist_2}{\E} \left[ \chi_{\vec{s}}(\vec{X_2}) \right]
				} \\
			&= \abs{ \underset{\vec{X_1} \sim \dist_1}{\E}
					\left[ \chi_{\vec{s}}(\vec{X_1}) \right] }
				\cdot \abs{ \underset{\vec{X_2} \sim \dist_2}{\E}
					\left[ \chi_{\vec{s}}(\vec{X_2}) \right] } \\
			&\leq \epsilon_1 \cdot \epsilon_2
		\end{align*}
		Where the third equality is by linearity of $\chi_{\vec{s}}$ and the fourth by independence of $\vec{X_1}$ and $\vec{X_2}$.
		The final inequality is due to our assumptions on $\dist_1$ and $\dist_2$.
	\end{proof}
	
	We get from this lemma a corollary which is "morally" equivalent to corollary \ref{corollary: XOR bias}.
	\begin{corollary} \label{corollary: ADD bias}
		If $\dist$ is $\epsilon$ biased w.r.t. $\chi_{\vec{s}}$ then:
		\begin{enumerate}
			\item $\ADD(\dist, \dist)$ is $\epsilon^2$-biased w.r.t. $\chi_{\vec{s}}$.
			\item For any distribution $\dist'$ defined over the same domain: $\ADD(\dist, \dist')$ is $\epsilon$-biased w.r.t. $\chi_{\vec{s}}$.
		\end{enumerate}
	\end{corollary}
	
	\begin{proof}
		Identical to the proof of corollary \ref{corollary: XOR bias}.
	\end{proof}
	
	\subsubsection{Increasing The Independence Parameter}
	
	With the bias-reduction function ready, we turn our attention to increasing the independence parameter $k$.
	This will be done in two stages.
	In the first stage we will show that an asymmetric function which receives two inputs from very different domains can be used to almost square the value of $k$ when given suitable distributions.
	In the second stage we will show how to generate these distributions from two samples from a $(k,\epsilon)$-biased hash family.
	
	\begin{definition}
		We define the function XIST (read: transist) as $\XIST : G \times \{0,1\} \rightarrow G$ for any group $G$ as:
		\begin{equation*}
			\XIST(x,b) =
			\begin{cases}
				x & b = 1 \\
				e & b = 0
			\end{cases}
		\end{equation*}
		Where $e$ is the neutral element of $G$.
		Specifically, when $G = \Z_q^k$: $e = \vec{0^k}$.
	\end{definition}
	
	\begin{note}
		The name XIST was chosen because this function is very similar to a transistor.
	\end{note}
	
	The intuition behind this definition is that whenever $b_i = 0$: a linear test on $\XIST(\vec{x},\vec{b})$ will not depend on $x_i$, which is very similar to what happened with the AND gate of lemma \ref{lemma: AND test enhancement}.
	
	We now define the properties we require a distribution over $\zo{k}$ to have in order to be useful to us when applying XIST.
	\begin{definition}
		Let $\B^p$ be any pairwise-independent probability distribution over $\zo{k}$ for arbitrary $k$ s.t. each bit $\B^p$ is a Bernoulli random variable with expectation $p$.
		We say a probability distribution $\B$ is an \emph{$(\epsilon_1, \epsilon_2)$-good} approximation of $\B^p$ if it has the two following properties:
		\begin{enumerate}
			\item For all $i \in \left[ k \right]$: $\B_i$ is $\epsilon_1$-close in $L_1$ norm to $\B^p_i$.
			\item For all pairs $i \neq j \in \left[ k \right]$: the pair $(\B_i, \B_j)$ is $\epsilon_2$-close in $L_1$ norm to $(\B^p_i, \B^p_j)$.
		\end{enumerate}
	\end{definition}
	
	\begin{lemma} \label{lemma: XIST test enhancement}
		Let $\dist$ be a $(t, \epsilon)$-biased distribution over $\Z_q^k$.
		Furthermore, let $\B$ be an $(\epsilon_1, \epsilon_2)$-good approximation of $\B^p$ defined over $\{0,1\}^k$, and assume $p > \epsilon_1$.
		Then $\XIST(\dist, \B)$ is $\nu + \epsilon(1 - \nu)$-biased w.r.t. all linear tests $\chi_{\vec{s}}$
			with weight $1 \leq \wt{\vec{s}} \leq \frac{t}{2 \left( p + \epsilon_1 \right)}$, for:
		\begin{equation*}
			\nu	= \frac{\wt{\vec{s}}\epsilon_2 + \epsilon_1 + p}{\wt{\vec{s}} \cdot (p - \epsilon_1)^2}
		\end{equation*}
	\end{lemma}
	
	\begin{note}
		We only require $\B$ to have small \emph{pairwise} correlations, while $\dist$ has small bias in any $t$-tuple of its components.
		This will enable us to achieve relatively high efficiency in our lemma.
	\end{note}
	
	\begin{proof}
		
		Let $\vec{s} \in \Z_q^k$ s.t. $t < \wt{\vec{s}} \leq \frac{1}{8}t^2$.
		
		For all $\vec{b} \in \zo{k}$ we denote by $\psi_{\vec{s}, \vec{b}} = \Supp{\vec{s}} \cap \Supp{\vec{b}} \in \left[k\right]$ the mutual support of $\vec{s}$ and $\vec{b}$.
		Then, by definitions:
		\begin{align*}
			\chi_{\vec{s}}(\XIST(\vec{x}, \vec{b}))
			&= e^{\frac{2 \pi i}{q} \sum_{j \in \left[ k \right]} s_j \cdot \XIST(x_j, b_j)} \\
			&= e^{\frac{2 \pi i}{q} \sum_{j \in \Supp{\vec{s}}} s_j \cdot \XIST(x_j, b_j)} \\
			&= e^{\frac{2 \pi i}{q} \sum_{j \in \psi_{\vec{s}, \vec{b}}} s_j \cdot x_j}
		\end{align*}
		

		In which the final equality follows from the definition of $p$ and the fact that $\Pr \left[ y^1_j = 0\right] = \frac{1}{2}$.
		
		If $\kappa_1 = \kappa_2$, then for $j = \kappa$: $z_1^{2^{j-1}} = z_2^{2^{j-1}} = -1$.
		Therefore, the expression can be bounded by:
		\begin{align*}
			\frac{p}{2} + \frac{1-p}{2} z_1^{2^{j-1}} + \frac{1-p}{2} z_2^{2^{j-1}} + \frac{p}{2} (z_1 \cdot z_2)^{2^{j-1}}
			&= p - (1 - p)\\
			&= 2p - 1 \\
			&\leq 2 \epsilon
		\end{align*}
	\end{proof}
	
\end{appendices}
\newpage \section*{Acknowledgements} \addcontentsline{toc}{section}{Acknowledgements}

	The insight that our methods can be used generically for any class of hash function families was suggested to us by Moni Naor.
	
	The idea to use the Valiant construction as a basis for PRF candidates with small-sized formulae was suggested by Yuval Ishai.
	
	Lemma \ref{lemma: random walk} was originally motivated by a construction proposed by Eyal Rosenman and Avi Wigderson.
	The version presented here is a generalization of a simpler and more efficient form that was suggested and proved to us by Amnon Ta-Shma.
	
	The proof of lemma \ref{lemma: XIST test enhancement} presented here is based on an improvement suggested by Prashant Vasudevan. This improvement allowed us to get better parameters for this lemma.

\newpage

\printbibliography[heading=bibintoc]

\newpage

\section*{\hfill \begin{hebrew}{}\end{hebrew}}
\begin{hebrew}
      $k$             $k$       $L_1$;     - .
       $k$ -,  "      $k$       $L_\infty$.

       -     .
,       $k$ - ( -),           $k'$ - ( -)  $k' > k$.
 ()                             $k$ -.
 ,       .

               .
        :          $L_\infty$      ;        - $k$.
,                 .
\end{hebrew}
\newpage

\begin{hebrew}
      .
         .
\end{hebrew}

\newpage

\begin{titlepage}
	\centering
	\includegraphics[width=0.4\textwidth]{IDClogoHebrew.png}\par\vspace{2cm}
	{\huge \begin{hebrew}{  }\end{hebrew} \par}
	{\Large \begin{hebrew}{    }\end{hebrew} \par}
	{\Large \begin{hebrew}{   (M.Sc.) -  }\end{hebrew} \par}
	
	\vspace{1cm}
	
	\vspace{1.5cm}
	{\Huge \begin{hebrew}     -  \end{hebrew} \par}
	\vspace{3cm}
	{\large \begin{hebrew}{}\end{hebrew}\par}
	{\large\bfseries \begin{hebrew}    \end{hebrew} \par}
	
	\vspace{2cm}
	
	{\begin{hebrew}{         M.Sc.}\end{hebrew}\par}
	{\begin{hebrew}{       ,   }\end{hebrew}\par}
	
	\vfill
	
	% Bottom of the page
	{\large\begin{hebrew}{ 2016}\end{hebrew}\par}
\end{titlepage}

\end{document}
